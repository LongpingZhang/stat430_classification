{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60d0ce9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.annotation_util import annotation_df\n",
    "from utils.dataset import CustomImageDataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import glob\n",
    "from model.model import CustomAlexNet\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae9fe04",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fcb3291",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "num_classes = 1\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5080da",
   "metadata": {},
   "source": [
    "# Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60d94872",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"data/\"\n",
    "train_dir = os.path.join(base_dir, \"Training\")\n",
    "val_dir = os.path.join(base_dir, \"Validate\")\n",
    "test_dir = os.path.join(base_dir, \"Testing\")\n",
    "df_train, df_val, df_test = annotation_df(train_dir, val_dir, test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edf15c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(17)\n",
    "data_aug = transforms.Compose([transforms.Resize((224,224))])\n",
    "\n",
    "train_dataset = CustomImageDataset(df_train, transform = data_aug)\n",
    "val_dataset = CustomImageDataset(df_val, transform = data_aug)\n",
    "test_dataset = CustomImageDataset(df_test, transform = data_aug)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = batch_size)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size = batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d99b80",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "137ca364",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/longpingzhang/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
    "model = CustomAlexNet(num_classes=num_classes, loss_fn=criterion, device=device, threshold=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08895ead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomAlexNet(\n",
       "  (pretrained_alexnet): AlexNet(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): ReLU(inplace=True)\n",
       "      (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (9): ReLU(inplace=True)\n",
       "      (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "    (classifier): Sequential(\n",
       "      (0): Dropout(p=0.5, inplace=False)\n",
       "      (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Dropout(p=0.5, inplace=False)\n",
       "      (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Linear(in_features=4096, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (loss_fn): BCEWithLogitsLoss()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6affb5cd",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96004b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "epochs = 10\n",
    "log_dir_base = os.path.join(os.getcwd(), 'logs')\n",
    "experiment_name = '1'\n",
    "ckpts_til_saving = 5\n",
    "start_training_from_ckpt = None\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "433b3aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logging directory for saving model weights and summary information\n",
    "log_dir = os.path.join(log_dir_base, experiment_name)\n",
    "Path(log_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_weights_dir = os.path.join(log_dir, \"checkpoints\")\n",
    "Path(model_weights_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "summary_dir = os.path.join(log_dir, \"summary\")\n",
    "Path(summary_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Start training from a specific checkpoint\n",
    "if start_training_from_ckpt:\n",
    "    model = model.load_state(torch.load(start_training_from_ckpt)['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "909665df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4149582/174996833.py:59: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  specificity = tn / (tn + fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epcoh: 0, training_loss: 1459.696004603676, validation_loss: 25374.98828125,               accuracy: 0.5, sensitivity: 0.5, specificity: nan \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4149582/174996833.py:59: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  specificity = tn / (tn + fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epcoh: 1, training_loss: 3053.5789935772236, validation_loss: 51465.4375,               accuracy: 0.5, sensitivity: 0.5, specificity: nan \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4149582/174996833.py:59: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  specificity = tn / (tn + fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epcoh: 2, training_loss: 5134.674697305148, validation_loss: 144879.65625,               accuracy: 0.5, sensitivity: 0.5, specificity: nan \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4149582/174996833.py:59: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  specificity = tn / (tn + fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epcoh: 3, training_loss: 9636.578275042686, validation_loss: 4578.7431640625,               accuracy: 0.5, sensitivity: 0.5, specificity: nan \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4149582/174996833.py:59: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  specificity = tn / (tn + fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epcoh: 4, training_loss: 1151.5689180542786, validation_loss: 5734.6337890625,               accuracy: 0.5, sensitivity: 0.5, specificity: nan \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4149582/174996833.py:59: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  specificity = tn / (tn + fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epcoh: 5, training_loss: 388.73219181941107, validation_loss: 73275.8671875,               accuracy: 0.5, sensitivity: 0.5, specificity: nan \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4149582/174996833.py:59: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  specificity = tn / (tn + fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epcoh: 6, training_loss: 4180.958160400391, validation_loss: 10140.6728515625,               accuracy: 0.5, sensitivity: 0.5, specificity: nan \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4149582/174996833.py:59: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  specificity = tn / (tn + fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epcoh: 7, training_loss: 659.8468995461097, validation_loss: 1546.4310302734375,               accuracy: 0.5, sensitivity: 0.5, specificity: nan \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4149582/174996833.py:59: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  specificity = tn / (tn + fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epcoh: 8, training_loss: 1079.8160876952684, validation_loss: 1857.92041015625,               accuracy: 0.5, sensitivity: 0.5, specificity: nan \n",
      "Epcoh: 9, training_loss: 166.6310195189256, validation_loss: 431.7682800292969,               accuracy: 0.5, sensitivity: 0.5, specificity: nan \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4149582/174996833.py:59: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  specificity = tn / (tn + fn)\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir=summary_dir ,flush_secs=20)\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "epoch_num = []\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    if epoch == 0:\n",
    "        torch.save({'model_state_dict': model.state_dict()},\n",
    "                    f'{model_weights_dir}/epoch{epoch}_before_training.pt')\n",
    "    \n",
    "    # Training step\n",
    "    iteration = 0\n",
    "    epoch_train_loss_it_cum = 0\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        train_loss = model.training_step(batch)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_train_loss_it_cum += train_loss.item()\n",
    "\n",
    "        iteration += 1         \n",
    "    epoch_train_loss = epoch_train_loss_it_cum / iteration\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    epoch_num.append(epoch)\n",
    "\n",
    "    # Validation step\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        val_loss, cf_matrix = model.validation_step(val_dataloader)\n",
    "        val_losses.append(val_loss.item())\n",
    "        \n",
    "        model.train()\n",
    "    \n",
    "    # Write to logs for tensorboard visualization\n",
    "    writer.add_scalars('alexnet', {'training_loss': epoch_train_loss,\n",
    "                                'validation_loss': val_loss}, epoch)\n",
    "    \n",
    "    # Save the model weights every ckpts_til_saving\n",
    "    if epochs % ckpts_til_saving == 0:\n",
    "        torch.save({'model_state_dict': model.state_dict()},\n",
    "                   f'{model_weights_dir}/epoch{epoch}.pt')\n",
    "    \n",
    "    # Save the best model\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save({'model_state_dict': model.state_dict()},\n",
    "                   f'{model_weights_dir}/best_model.pt')\n",
    "        \n",
    "    # Calculate accuracy, sensitivity, and specificity over validation set\n",
    "    tn, fp, fn, tp = cf_matrix.ravel()\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "#     sensitivity = tp / (tp + fp)\n",
    "#     specificity = tn / (tn + fn)\n",
    "    \n",
    "    print(f'Epcoh: {epoch}, training_loss: {epoch_train_loss}, validation_loss: {val_loss}, \\\n",
    "              accuracy: {accuracy}, sensitivity: {sensitivity}, specificity: {specificity} ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:opence-v1.6.1]",
   "language": "python",
   "name": "conda-env-opence-v1.6.1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
