{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0Jn75mXYwf5",
        "outputId": "10ec3d19-1fbc-42c5-e554-f016d4a9d96a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'stat430_classification'...\n",
            "remote: Enumerating objects: 644, done.\u001b[K\n",
            "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 644 (delta 5), reused 23 (delta 4), pack-reused 620\u001b[K\n",
            "Receiving objects: 100% (644/644), 191.00 MiB | 26.45 MiB/s, done.\n",
            "Resolving deltas: 100% (5/5), done.\n",
            "Checking out files: 100% (616/616), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/LongpingZhang/stat430_classification.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4F8ZLyAZSI1"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('stat430_classification/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NV82kOSnY_xS"
      },
      "outputs": [],
      "source": [
        "from utils.annotation_util import annotation_df\n",
        "from utils.dataset import CustomImageDataset\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import glob\n",
        "from model.model import CustomNetwork\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from pathlib import Path\n",
        "from copy import deepcopy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vgjo2TemaVK9"
      },
      "outputs": [],
      "source": [
        "batch_size = 10\n",
        "num_classes = 1\n",
        "epochs = 25\n",
        "criterion = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "project_dir = \"stat430_classification/\"\n",
        "base_dir = os.path.join(project_dir, \"data/\")\n",
        "train_dir = os.path.join(base_dir, \"Training\")\n",
        "val_dir = os.path.join(base_dir, \"Validate\")\n",
        "test_dir = os.path.join(base_dir, \"Testing\")\n",
        "df_train, df_val, df_test = annotation_df(train_dir, val_dir, test_dir)\n",
        "\n",
        "torch.manual_seed(17)\n",
        "data_aug = transforms.Compose([transforms.Resize((224,224))])\n",
        "train_dataset = CustomImageDataset(df_train, transform = data_aug)\n",
        "val_dataset = CustomImageDataset(df_val, transform = data_aug)\n",
        "test_dataset = CustomImageDataset(df_test, transform = data_aug)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size = batch_size)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size = batch_size)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size = batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SuPpBynqifYM"
      },
      "outputs": [],
      "source": [
        "!rm -rf logs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6NfMRd1PsCg",
        "outputId": "b31344de-3003-4d66-d5b6-8c20ae008053"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Method: alexnet, epcoh: 0, training_loss: 1.0978629016323682e+19, validation_loss: 9317301354496.0, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 1, training_loss: 6406733927195.385, validation_loss: 560413439688704.0, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 2, training_loss: 3001854052457.1104, validation_loss: 9578061234176.0, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 3, training_loss: 412256927097.2308, validation_loss: 40166982942720.0, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 4, training_loss: 1033384226887.3846, validation_loss: 241449572499456.0, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 5, training_loss: 1689103214542.7693, validation_loss: 31920415047680.0, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 6, training_loss: 188749033738.6154, validation_loss: 518939893301248.0, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 7, training_loss: 2040833007903.4326, validation_loss: 129923532455936.0, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 8, training_loss: 301087481016.7211, validation_loss: 40076931235840.0, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 9, training_loss: 295495568919.5961, validation_loss: 58175466242048.0, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 10, training_loss: 175144054110.69232, validation_loss: 11541263941632.0, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 11, training_loss: 10718835124.923077, validation_loss: 18550009888768.0, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 12, training_loss: 128072964174.76923, validation_loss: 25307675361280.0, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 13, training_loss: 40189715368.34615, validation_loss: 19977444786176.0, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 14, training_loss: 9505698169.153847, validation_loss: 8746312400896.0, accuracy: 0.5625\n",
            "Method: alexnet, epcoh: 15, training_loss: 133946210211.84134, validation_loss: 485876531200.0, accuracy: 0.53125\n",
            "Method: alexnet, epcoh: 16, training_loss: 80454819.67581412, validation_loss: 811504500736.0, accuracy: 0.53125\n",
            "Method: alexnet, epcoh: 17, training_loss: 6757384.498065655, validation_loss: 1108315471872.0, accuracy: 0.59375\n",
            "Method: alexnet, epcoh: 18, training_loss: 1846992549.220265, validation_loss: 1254697074688.0, accuracy: 0.625\n",
            "Method: alexnet, epcoh: 19, training_loss: 1225126.9349778248, validation_loss: 1247107219456.0, accuracy: 0.625\n",
            "Method: alexnet, epcoh: 20, training_loss: 330.30158947064325, validation_loss: 1223788331008.0, accuracy: 0.625\n",
            "Method: alexnet, epcoh: 21, training_loss: 484449.7636709213, validation_loss: 1204767817728.0, accuracy: 0.625\n",
            "Method: alexnet, epcoh: 22, training_loss: 381.2003907148655, validation_loss: 1175550164992.0, accuracy: 0.625\n",
            "Method: alexnet, epcoh: 23, training_loss: 574.9317243145063, validation_loss: 1207821008896.0, accuracy: 0.625\n",
            "Method: alexnet, epcoh: 24, training_loss: 292.8701910911878, validation_loss: 1228968689664.0, accuracy: 0.625\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Method: alexnet, epcoh: 0, training_loss: 32986796347.1911, validation_loss: 440791392.0, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 1, training_loss: 2025389353.2157452, validation_loss: 239587968.0, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 2, training_loss: 21421957.99215933, validation_loss: 557099968.0, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 3, training_loss: 53909518.76242975, validation_loss: 67381.09375, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 4, training_loss: 9023.2035108713, validation_loss: 16554.728515625, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 5, training_loss: 2959.9400349396924, validation_loss: 6471.3173828125, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 6, training_loss: 1250.3557558549512, validation_loss: 1537.923095703125, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 7, training_loss: 393.8301139153683, validation_loss: 1250.466064453125, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 8, training_loss: 177.26471884928358, validation_loss: 144.5393829345703, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 9, training_loss: 14.406739280129282, validation_loss: 160.80789184570312, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 10, training_loss: 12.928472934147486, validation_loss: 59.03443908691406, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 11, training_loss: 15.987803190516738, validation_loss: 0.6743573546409607, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 12, training_loss: 18.28394468830755, validation_loss: 0.7224442958831787, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 13, training_loss: 0.8765826018956991, validation_loss: 0.679479718208313, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 14, training_loss: 0.7287319027460538, validation_loss: 0.6821655035018921, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 15, training_loss: 0.7248768485509433, validation_loss: 0.6833186149597168, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 16, training_loss: 0.7402603981586603, validation_loss: 0.6836861968040466, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 17, training_loss: 0.7245287104294851, validation_loss: 0.6837531328201294, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 18, training_loss: 0.7227519475496732, validation_loss: 0.6838889122009277, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 19, training_loss: 1.4793545340116208, validation_loss: 0.6781731247901917, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 20, training_loss: 0.7264846723813277, validation_loss: 0.6803282499313354, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 21, training_loss: 0.722770230128215, validation_loss: 0.6814972758293152, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 22, training_loss: 0.7212286671766868, validation_loss: 0.6822724342346191, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 23, training_loss: 0.7202447847678111, validation_loss: 0.6827909350395203, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 24, training_loss: 0.7195701232323279, validation_loss: 0.6831439733505249, accuracy: 0.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Method: alexnet, epcoh: 0, training_loss: 405.41262411691537, validation_loss: 123338.015625, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 1, training_loss: 8125.914536971312, validation_loss: 50677.390625, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 2, training_loss: 3756.9160175759057, validation_loss: 13234.2099609375, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 3, training_loss: 963.449623936644, validation_loss: 10427.7392578125, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 4, training_loss: 780.6791548063472, validation_loss: 15115.955078125, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 5, training_loss: 954.2982205863183, validation_loss: 1851.060791015625, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 6, training_loss: 159.25068883941108, validation_loss: 1095.473876953125, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 7, training_loss: 72.06444875287005, validation_loss: 995.644287109375, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 8, training_loss: 67.61432338092453, validation_loss: 523.9483642578125, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 9, training_loss: 33.68214192653816, validation_loss: 392.6579284667969, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 10, training_loss: 27.35534247899918, validation_loss: 149.93203735351562, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 11, training_loss: 4.8026582609739785, validation_loss: 214.21994018554688, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 12, training_loss: 14.517891545362083, validation_loss: 29.664453506469727, accuracy: 0.5\n",
            "Method: alexnet, epcoh: 13, training_loss: 2.568818822911663, validation_loss: 0.7197039723396301, accuracy: 0.53125\n",
            "Method: alexnet, epcoh: 14, training_loss: 0.8964930021835383, validation_loss: 0.8113137483596802, accuracy: 0.59375\n",
            "Method: alexnet, epcoh: 15, training_loss: 0.5896126956840118, validation_loss: 1.871634602546692, accuracy: 0.625\n",
            "Method: alexnet, epcoh: 16, training_loss: 0.48686097406607587, validation_loss: 22.103073120117188, accuracy: 0.53125\n",
            "Method: alexnet, epcoh: 17, training_loss: 2.051760792957234, validation_loss: 0.8517189025878906, accuracy: 0.53125\n",
            "Method: alexnet, epcoh: 18, training_loss: 0.6403677965729282, validation_loss: 0.5150609612464905, accuracy: 0.625\n",
            "Method: alexnet, epcoh: 19, training_loss: 0.4926764103942193, validation_loss: 0.5260054469108582, accuracy: 0.71875\n",
            "Method: alexnet, epcoh: 20, training_loss: 0.40878846413957387, validation_loss: 1.7818273305892944, accuracy: 0.625\n",
            "Method: alexnet, epcoh: 21, training_loss: 0.36537188985609576, validation_loss: 1.88837468624115, accuracy: 0.59375\n",
            "Method: alexnet, epcoh: 22, training_loss: 0.6428148423637339, validation_loss: 2.762709856033325, accuracy: 0.65625\n",
            "Method: alexnet, epcoh: 23, training_loss: 0.46560655633227443, validation_loss: 1.3579217195510864, accuracy: 0.75\n",
            "Method: alexnet, epcoh: 24, training_loss: 0.27345173806399825, validation_loss: 3.0895230770111084, accuracy: 0.65625\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG11_Weights.IMAGENET1K_V1`. You can also use `weights=VGG11_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Method: vgg11, epcoh: 0, training_loss: 2.722778435886305e+27, validation_loss: 3.190071305678224e+27, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 1, training_loss: 2.586323947911489e+27, validation_loss: 3.017751267274595e+27, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 2, training_loss: 2.3289592294962493e+26, validation_loss: 2.7056496236998823e+26, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 3, training_loss: 8.134612973034071e+25, validation_loss: 8.96538970929793e+25, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 4, training_loss: 7.653952280118011e+24, validation_loss: 4.2620735249096553e+24, accuracy: 0.4375\n",
            "Method: vgg11, epcoh: 5, training_loss: 5.433861182391295e+24, validation_loss: 2.61696348891787e+24, accuracy: 0.34375\n",
            "Method: vgg11, epcoh: 6, training_loss: 4.763290812895028e+24, validation_loss: 1.918759125644358e+24, accuracy: 0.34375\n",
            "Method: vgg11, epcoh: 7, training_loss: 4.381637977395894e+24, validation_loss: 1.7721733708993178e+24, accuracy: 0.3125\n",
            "Method: vgg11, epcoh: 8, training_loss: 4.2131710594836175e+24, validation_loss: 1.7293308077881274e+24, accuracy: 0.3125\n",
            "Method: vgg11, epcoh: 9, training_loss: 4.1972780386211964e+24, validation_loss: 1.6633913358295838e+24, accuracy: 0.375\n",
            "Method: vgg11, epcoh: 10, training_loss: 4.0276101295140436e+24, validation_loss: 1.460481330359233e+24, accuracy: 0.40625\n",
            "Method: vgg11, epcoh: 11, training_loss: 3.767210925124186e+24, validation_loss: 1.3342728867473663e+24, accuracy: 0.4375\n",
            "Method: vgg11, epcoh: 12, training_loss: 3.7288708738478405e+24, validation_loss: 1.2140447910950836e+24, accuracy: 0.46875\n",
            "Method: vgg11, epcoh: 13, training_loss: 4.156936154632022e+24, validation_loss: 1.1755853473140322e+24, accuracy: 0.46875\n",
            "Method: vgg11, epcoh: 14, training_loss: 3.0940126566151334e+24, validation_loss: 1.0413731064444028e+24, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 15, training_loss: 3.280805412323989e+24, validation_loss: 1.0109575239433994e+24, accuracy: 0.53125\n",
            "Method: vgg11, epcoh: 16, training_loss: 2.9252872152390505e+24, validation_loss: 9.063155579391145e+23, accuracy: 0.59375\n",
            "Method: vgg11, epcoh: 17, training_loss: 3.2666134200945167e+24, validation_loss: 8.710352231494225e+23, accuracy: 0.625\n",
            "Method: vgg11, epcoh: 18, training_loss: 2.714973569082591e+24, validation_loss: 9.239041681254264e+23, accuracy: 0.625\n",
            "Method: vgg11, epcoh: 19, training_loss: 3.2452272137853274e+24, validation_loss: 9.340989485875064e+23, accuracy: 0.59375\n",
            "Method: vgg11, epcoh: 20, training_loss: 2.44568242645299e+24, validation_loss: 9.031355842566267e+23, accuracy: 0.6875\n",
            "Method: vgg11, epcoh: 21, training_loss: 2.867792442248928e+24, validation_loss: 9.409254688738776e+23, accuracy: 0.59375\n",
            "Method: vgg11, epcoh: 22, training_loss: 2.5563456005369856e+24, validation_loss: 7.527819379193432e+23, accuracy: 0.71875\n",
            "Method: vgg11, epcoh: 23, training_loss: 1.9023254632360547e+24, validation_loss: 1.6468957672872332e+24, accuracy: 0.53125\n",
            "Method: vgg11, epcoh: 24, training_loss: 2.7716815733076073e+24, validation_loss: 9.018334314747673e+23, accuracy: 0.59375\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG11_Weights.IMAGENET1K_V1`. You can also use `weights=VGG11_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Method: vgg11, epcoh: 0, training_loss: 452806895428214.25, validation_loss: 2285431685120.0, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 1, training_loss: 7118491723815.385, validation_loss: 287377489920.0, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 2, training_loss: 83295991477.11539, validation_loss: 189507141632.0, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 3, training_loss: 32393032072.923077, validation_loss: 795451457536.0, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 4, training_loss: 49575290496.61539, validation_loss: 4560942592.0, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 5, training_loss: 2702976898667.6924, validation_loss: 1383715045376.0, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 6, training_loss: 150106569235.69232, validation_loss: 15249518362624.0, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 7, training_loss: 917871821968.0, validation_loss: 63801581568.0, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 8, training_loss: 4550495393.961538, validation_loss: 11490113536.0, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 9, training_loss: 3462970400.230769, validation_loss: 3781451776.0, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 10, training_loss: 589456399.7067307, validation_loss: 2006322176.0, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 11, training_loss: 168590873.25480768, validation_loss: 2920932608.0, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 12, training_loss: 198740138.42382812, validation_loss: 111453552.0, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 13, training_loss: 8066228.4547025245, validation_loss: 6828273.0, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 14, training_loss: 1650030.2288254958, validation_loss: 11600900.0, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 15, training_loss: 1653595.3966346155, validation_loss: 5979700.0, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 16, training_loss: 2340344.3899113582, validation_loss: 709304.5, accuracy: 0.53125\n",
            "Method: vgg11, epcoh: 17, training_loss: 521497.13987379806, validation_loss: 1981954.5, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 18, training_loss: 536551.0626878005, validation_loss: 5530749.0, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 19, training_loss: 833059.4778911883, validation_loss: 2440118.5, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 20, training_loss: 627333.4642055219, validation_loss: 1165640.25, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 21, training_loss: 162091.24462890625, validation_loss: 9669466.0, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 22, training_loss: 884577.00783128, validation_loss: 9148481.0, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 23, training_loss: 782801.9762056791, validation_loss: 544529856.0, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 24, training_loss: 32891115.487830527, validation_loss: 70390256.0, accuracy: 0.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG11_Weights.IMAGENET1K_V1`. You can also use `weights=VGG11_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Method: vgg11, epcoh: 0, training_loss: 5860.79249059237, validation_loss: 183882.46875, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 1, training_loss: 11610.586405313932, validation_loss: 42125.4921875, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 2, training_loss: 2510.9661285258258, validation_loss: 14926.1455078125, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 3, training_loss: 1345.584905220912, validation_loss: 36200.8046875, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 4, training_loss: 1903.4955755380483, validation_loss: 5415.2724609375, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 5, training_loss: 459.3778746128082, validation_loss: 5885.0791015625, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 6, training_loss: 464.3881662866513, validation_loss: 10417.59765625, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 7, training_loss: 672.0488331806201, validation_loss: 9772.900390625, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 8, training_loss: 786.514889056866, validation_loss: 4605.71484375, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 9, training_loss: 305.17506031278305, validation_loss: 10388.8232421875, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 10, training_loss: 613.6834917070727, validation_loss: 4229.4716796875, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 11, training_loss: 256.56657636017167, validation_loss: 79.20723724365234, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 12, training_loss: 10.622641554265558, validation_loss: 175.24378967285156, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 13, training_loss: 32.32110611509423, validation_loss: 1163.5784912109375, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 14, training_loss: 113.03941753992102, validation_loss: 630.3065185546875, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 15, training_loss: 39.33533568050777, validation_loss: 60.25537872314453, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 16, training_loss: 9.805385677110637, validation_loss: 43.959693908691406, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 17, training_loss: 6.278394509523315, validation_loss: 135.39620971679688, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 18, training_loss: 9.509031946546354, validation_loss: 30.861968994140625, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 19, training_loss: 10.162586717716685, validation_loss: 49.52680206298828, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 20, training_loss: 9.443159050621292, validation_loss: 114.11383056640625, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 21, training_loss: 13.07675408355473, validation_loss: 12.048151969909668, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 22, training_loss: 2.005264078568089, validation_loss: 41.823997497558594, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 23, training_loss: 3.9596577109979574, validation_loss: 6.560647964477539, accuracy: 0.5\n",
            "Method: vgg11, epcoh: 24, training_loss: 2.296137341431145, validation_loss: 3.0892179012298584, accuracy: 0.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Method: resnet18, epcoh: 0, training_loss: 16.575418563320106, validation_loss: 20.831039428710938, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 1, training_loss: 5.09189942453304, validation_loss: 165.57723999023438, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 2, training_loss: 4.192355534654871, validation_loss: 172.0626983642578, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 3, training_loss: 4.55914563244499, validation_loss: 8.745125770568848, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 4, training_loss: 4.405631171926002, validation_loss: 4.328950881958008, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 5, training_loss: 2.820039151435637, validation_loss: 18.067476272583008, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 6, training_loss: 5.098500187750295, validation_loss: 83.21969604492188, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 7, training_loss: 5.471969490570446, validation_loss: 4.010058403015137, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 8, training_loss: 3.9068411493707567, validation_loss: 37.08466720581055, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 9, training_loss: 1.050593358838759, validation_loss: 48.417137145996094, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 10, training_loss: 9.812446529432528, validation_loss: 21.191017150878906, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 11, training_loss: 10.113767637613403, validation_loss: 1926.824462890625, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 12, training_loss: 23.55297703530605, validation_loss: 227.05722045898438, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 13, training_loss: 15.95470901657421, validation_loss: 16.08161163330078, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 14, training_loss: 6.093883549644673, validation_loss: 1264.1363525390625, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 15, training_loss: 9.868137812976146, validation_loss: 530.6240844726562, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 16, training_loss: 9.137106545080583, validation_loss: 528.2359619140625, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 17, training_loss: 10.707012100625544, validation_loss: 672.140380859375, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 18, training_loss: 9.902887409703835, validation_loss: 21.121105194091797, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 19, training_loss: 2.9543353087053847, validation_loss: 49.32227325439453, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 20, training_loss: 1.0500150512044246, validation_loss: 15.982245445251465, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 21, training_loss: 0.9168019561240306, validation_loss: 17.94741439819336, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 22, training_loss: 0.9951259791851044, validation_loss: 3.203303098678589, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 23, training_loss: 0.8933115320710036, validation_loss: 3.481027603149414, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 24, training_loss: 0.8912727001767892, validation_loss: 3.5772225856781006, accuracy: 0.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Method: resnet18, epcoh: 0, training_loss: 1.2929516525295084, validation_loss: 17683.666015625, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 1, training_loss: 4.212694412476804, validation_loss: 1.1532440185546875, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 2, training_loss: 1.0190181457079375, validation_loss: 15.427924156188965, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 3, training_loss: 0.9253433341017137, validation_loss: 17.67323112487793, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 4, training_loss: 0.8278463213489606, validation_loss: 0.9652955532073975, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 5, training_loss: 0.7973280720985852, validation_loss: 0.7313010096549988, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 6, training_loss: 0.7204252790946227, validation_loss: 0.6678750514984131, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 7, training_loss: 0.7067021045547265, validation_loss: 0.6746976375579834, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 8, training_loss: 0.7008995703206613, validation_loss: 0.6785650253295898, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 9, training_loss: 0.653731594256197, validation_loss: 1.109519600868225, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 10, training_loss: 0.9286423075288632, validation_loss: 21.3798770904541, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 11, training_loss: 0.7409209769505721, validation_loss: 0.7037444710731506, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 12, training_loss: 0.7198881931029834, validation_loss: 0.6796268820762634, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 13, training_loss: 0.7133729893427628, validation_loss: 0.6796776056289673, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 14, training_loss: 0.7120493432650199, validation_loss: 0.6787087917327881, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 15, training_loss: 0.7116783490547767, validation_loss: 0.6779785752296448, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 16, training_loss: 0.7105459582347137, validation_loss: 0.6757509708404541, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 17, training_loss: 0.7101818070961878, validation_loss: 0.6736080050468445, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 18, training_loss: 0.7093434689136652, validation_loss: 0.6698176860809326, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 19, training_loss: 0.7086743437326871, validation_loss: 0.6663976907730103, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 20, training_loss: 0.7076861033072839, validation_loss: 0.6636294722557068, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 21, training_loss: 0.706715355698879, validation_loss: 0.6701942086219788, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 22, training_loss: 0.7045559974817129, validation_loss: 0.6862287521362305, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 23, training_loss: 0.6900115861342504, validation_loss: 1.0315316915512085, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 24, training_loss: 0.7889733910560608, validation_loss: 0.6644569635391235, accuracy: 0.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Method: resnet18, epcoh: 0, training_loss: 0.6432342769101245, validation_loss: 5.838571548461914, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 1, training_loss: 1.4077180109631557, validation_loss: 0.8475325107574463, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 2, training_loss: 1.1311612576246262, validation_loss: 0.6919293403625488, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 3, training_loss: 1.0030908223528128, validation_loss: 0.7144464254379272, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 4, training_loss: 0.9733205431929002, validation_loss: 0.6317487955093384, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 5, training_loss: 0.9406625826198322, validation_loss: 0.6932646036148071, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 6, training_loss: 0.9825828745961189, validation_loss: 0.6884805560112, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 7, training_loss: 0.9374059066176414, validation_loss: 0.6753637790679932, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 8, training_loss: 0.9100435490791614, validation_loss: 0.6857782006263733, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 9, training_loss: 0.898419934969682, validation_loss: 0.681219220161438, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 10, training_loss: 0.8705963251682428, validation_loss: 0.6754792332649231, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 11, training_loss: 0.8434373037173197, validation_loss: 0.6738885641098022, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 12, training_loss: 0.8508033557580068, validation_loss: 0.681104302406311, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 13, training_loss: 0.8566512654607112, validation_loss: 0.7051194906234741, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 14, training_loss: 0.8767449437425687, validation_loss: 0.7130511999130249, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 15, training_loss: 0.8425588837036719, validation_loss: 0.7006571888923645, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 16, training_loss: 0.8363554019194382, validation_loss: 0.7025979161262512, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 17, training_loss: 0.8552995175123215, validation_loss: 0.696776270866394, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 18, training_loss: 0.844360565336851, validation_loss: 0.707371711730957, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 19, training_loss: 0.8247149214148521, validation_loss: 0.6822290420532227, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 20, training_loss: 0.8070530478770916, validation_loss: 0.6790561079978943, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 21, training_loss: 0.791652921300668, validation_loss: 0.6730378866195679, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 22, training_loss: 0.8100339947984769, validation_loss: 0.6908990740776062, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 23, training_loss: 0.8079655984273324, validation_loss: 0.6793039441108704, accuracy: 0.5\n",
            "Method: resnet18, epcoh: 24, training_loss: 0.773807497551808, validation_loss: 0.7047134637832642, accuracy: 0.5\n"
          ]
        }
      ],
      "source": [
        "df_results = []\n",
        "\n",
        "for backbone in ['alexnet', 'vgg11', 'resnet18']:\n",
        "    for learning_rate in [1e-1, 1e-2, 1e-3]:\n",
        "        model = CustomNetwork(num_classes=num_classes, loss_fn=criterion, device=device, threshold=0.5, backbone=backbone).to(device)\n",
        "        log_dir_base = os.path.join(os.getcwd(), 'logs')\n",
        "        experiment_name = '1'\n",
        "        ckpts_til_saving = 5\n",
        "        start_training_from_ckpt = None\n",
        "        optimizer = torch.optim.Adam(\n",
        "            model.parameters(), lr=learning_rate\n",
        "        )\n",
        "\n",
        "        log_dir = os.path.join(log_dir_base, experiment_name, backbone, 'lr_'+str(learning_rate))\n",
        "        Path(log_dir).mkdir(parents=True, exist_ok=True)\n",
        "        model_weights_dir = os.path.join(log_dir, \"checkpoints\")\n",
        "        Path(model_weights_dir).mkdir(parents=True, exist_ok=True)\n",
        "        summary_dir = os.path.join(log_dir, \"summary\")\n",
        "        Path(summary_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        if start_training_from_ckpt:\n",
        "            model = model.load_state_dict(torch.load(start_training_from_ckpt)['model_state_dict'])\n",
        "        \n",
        "        writer = SummaryWriter(log_dir=summary_dir ,flush_secs=20)\n",
        "        best_loss = float(\"inf\")\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        epoch_num = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            if epoch == 0:\n",
        "                torch.save({'model_state_dict': model.state_dict()},\n",
        "                            f'{model_weights_dir}/epoch{epoch}_before_training.pt')\n",
        "                \n",
        "            iteration = 0\n",
        "            epoch_train_loss_it_cum = 0\n",
        "            model.train()\n",
        "    \n",
        "            for batch in train_dataloader:\n",
        "                optimizer.zero_grad()\n",
        "                train_loss = model.training_step(batch)\n",
        "                train_loss.backward()\n",
        "                optimizer.step()\n",
        "                epoch_train_loss_it_cum += train_loss.item()\n",
        "\n",
        "                iteration += 1         \n",
        "            epoch_train_loss = epoch_train_loss_it_cum / iteration\n",
        "            train_losses.append(epoch_train_loss)\n",
        "            epoch_num.append(epoch)\n",
        "\n",
        "            # Validation step\n",
        "            with torch.no_grad():\n",
        "                model.eval()\n",
        "                val_loss, cf_matrix = model.validation_step(val_dataloader)\n",
        "                val_losses.append(val_loss.item())\n",
        "                \n",
        "                model.train()\n",
        "            \n",
        "            # Write to logs for tensorboard visualization\n",
        "            writer.add_scalars('alexnet', {'training_loss': epoch_train_loss,\n",
        "                                        'validation_loss': val_loss}, epoch)\n",
        "            \n",
        "            # Save the model weights every ckpts_til_saving\n",
        "            if epochs % ckpts_til_saving == 0:\n",
        "                torch.save({'model_state_dict': model.state_dict()},\n",
        "                          f'{model_weights_dir}/epoch{epoch}.pt')\n",
        "            \n",
        "            # Save the best model\n",
        "            if val_loss < best_loss:\n",
        "                best_loss = val_loss\n",
        "                torch.save({'model_state_dict': model.state_dict()},\n",
        "                          f'{model_weights_dir}/best_model.pt')\n",
        "                \n",
        "            # Calculate accuracy, sensitivity, and specificity over validation set\n",
        "            tn, fp, fn, tp = cf_matrix.ravel()\n",
        "            accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "\n",
        "            print(f'Method: {backbone}, epcoh: {epoch}, training_loss: {epoch_train_loss}, validation_loss: {val_loss}, accuracy: {accuracy}')\n",
        "        \n",
        "        # Testing step\n",
        "        with torch.no_grad():\n",
        "            best_model = deepcopy(model)\n",
        "            best_model.load_state_dict(torch.load(f'{model_weights_dir}/best_model.pt')['model_state_dict'])\n",
        "            best_model.eval()\n",
        "            test_loss, cf_matrix = model.testing_step(test_dataloader)\n",
        "        tn, fp, fn, tp = cf_matrix.ravel()\n",
        "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "        # precision = tp / (tp + fp)\n",
        "        # recall = tp / (tp + fn)\n",
        "        sensitivity = tp / (tp + fn)\n",
        "        specificity = tn / (tn + fp)\n",
        "        df_results.append({'Method': backbone, \n",
        "                           'Learning_rate': learning_rate, \n",
        "                           'test_loss': test_loss.item(), \n",
        "                           'accuracy': accuracy,\n",
        "                           'sensitivity': sensitivity,\n",
        "                           'specificity': specificity})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "D_1uwPupekr_",
        "outputId": "3a31bcbd-8704-4ac1-b3c2-47b39869827a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-3d455dd8-c9f5-4d4a-a2f7-2c9579974922\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Method</th>\n",
              "      <th>Learning_rate</th>\n",
              "      <th>test_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>sensitivity</th>\n",
              "      <th>specificity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>alexnet</td>\n",
              "      <td>0.100</td>\n",
              "      <td>1.068900e+02</td>\n",
              "      <td>0.531250</td>\n",
              "      <td>0.62500</td>\n",
              "      <td>0.43750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>alexnet</td>\n",
              "      <td>0.010</td>\n",
              "      <td>6.898298e-01</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>alexnet</td>\n",
              "      <td>0.001</td>\n",
              "      <td>4.659916e+00</td>\n",
              "      <td>0.703125</td>\n",
              "      <td>0.96875</td>\n",
              "      <td>0.43750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>vgg11</td>\n",
              "      <td>0.100</td>\n",
              "      <td>3.688562e+24</td>\n",
              "      <td>0.546875</td>\n",
              "      <td>0.81250</td>\n",
              "      <td>0.28125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>vgg11</td>\n",
              "      <td>0.010</td>\n",
              "      <td>8.879454e+07</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>vgg11</td>\n",
              "      <td>0.001</td>\n",
              "      <td>3.328768e+00</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>0.100</td>\n",
              "      <td>7.252467e-01</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>0.010</td>\n",
              "      <td>6.870048e-01</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>resnet18</td>\n",
              "      <td>0.001</td>\n",
              "      <td>7.893061e-01</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.00000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3d455dd8-c9f5-4d4a-a2f7-2c9579974922')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3d455dd8-c9f5-4d4a-a2f7-2c9579974922 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3d455dd8-c9f5-4d4a-a2f7-2c9579974922');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     Method  Learning_rate     test_loss  accuracy  sensitivity  specificity\n",
              "0   alexnet          0.100  1.068900e+02  0.531250      0.62500      0.43750\n",
              "1   alexnet          0.010  6.898298e-01  0.500000      1.00000      0.00000\n",
              "2   alexnet          0.001  4.659916e+00  0.703125      0.96875      0.43750\n",
              "3     vgg11          0.100  3.688562e+24  0.546875      0.81250      0.28125\n",
              "4     vgg11          0.010  8.879454e+07  0.500000      1.00000      0.00000\n",
              "5     vgg11          0.001  3.328768e+00  0.500000      1.00000      0.00000\n",
              "6  resnet18          0.100  7.252467e-01  0.500000      1.00000      0.00000\n",
              "7  resnet18          0.010  6.870048e-01  0.500000      1.00000      0.00000\n",
              "8  resnet18          0.001  7.893061e-01  0.500000      1.00000      0.00000"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "pd.DataFrame(df_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tzi13nrik8sN",
        "outputId": "4655ccab-548d-4f69-9552-514922f01048"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gradio\n",
            "  Downloading gradio-3.13.0-py3-none-any.whl (13.8 MB)\n",
            "\u001b[K     || 13.8 MB 9.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from gradio) (7.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from gradio) (2022.11.0)\n",
            "Collecting websockets>=10.0\n",
            "  Downloading websockets-10.4-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[K     || 106 kB 43.1 MB/s \n",
            "\u001b[?25hCollecting fastapi\n",
            "  Downloading fastapi-0.88.0-py3-none-any.whl (55 kB)\n",
            "\u001b[K     || 55 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from gradio) (3.8.3)\n",
            "Collecting orjson\n",
            "  Downloading orjson-3.8.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (278 kB)\n",
            "\u001b[K     || 278 kB 76.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pydantic in /usr/local/lib/python3.8/dist-packages (from gradio) (1.10.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from gradio) (3.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from gradio) (2.23.0)\n",
            "Collecting python-multipart\n",
            "  Downloading python-multipart-0.0.5.tar.gz (32 kB)\n",
            "Collecting ffmpy\n",
            "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
            "Collecting pycryptodome\n",
            "  Downloading pycryptodome-3.16.0-cp35-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.3 MB)\n",
            "\u001b[K     || 2.3 MB 65.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from gradio) (6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from gradio) (2.11.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from gradio) (1.3.5)\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.20.0-py3-none-any.whl (56 kB)\n",
            "\u001b[K     || 56 kB 2.9 MB/s \n",
            "\u001b[?25hCollecting markdown-it-py[linkify,plugins]\n",
            "  Downloading markdown_it_py-2.1.0-py3-none-any.whl (84 kB)\n",
            "\u001b[K     || 84 kB 1.8 MB/s \n",
            "\u001b[?25hCollecting h11<0.13,>=0.11\n",
            "  Downloading h11-0.12.0-py3-none-any.whl (54 kB)\n",
            "\u001b[K     || 54 kB 2.7 MB/s \n",
            "\u001b[?25hCollecting httpx\n",
            "  Downloading httpx-0.23.1-py3-none-any.whl (84 kB)\n",
            "\u001b[K     || 84 kB 2.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from gradio) (1.21.6)\n",
            "Collecting paramiko\n",
            "  Downloading paramiko-2.12.0-py2.py3-none-any.whl (213 kB)\n",
            "\u001b[K     || 213 kB 53.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (22.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (6.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (1.8.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (2.1.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (1.3.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (4.0.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.8/dist-packages (from yarl<2.0,>=1.0->aiohttp->gradio) (2.10)\n",
            "Collecting starlette==0.22.0\n",
            "  Downloading starlette-0.22.0-py3-none-any.whl (64 kB)\n",
            "\u001b[K     || 64 kB 1.8 MB/s \n",
            "\u001b[?25hCollecting anyio<5,>=3.4.0\n",
            "  Downloading anyio-3.6.2-py3-none-any.whl (80 kB)\n",
            "\u001b[K     || 80 kB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.8/dist-packages (from starlette==0.22.0->fastapi->gradio) (4.4.0)\n",
            "Collecting sniffio>=1.1\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Collecting httpcore<0.17.0,>=0.15.0\n",
            "  Downloading httpcore-0.16.2-py3-none-any.whl (68 kB)\n",
            "\u001b[K     || 68 kB 3.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from httpx->gradio) (2022.9.24)\n",
            "Collecting rfc3986[idna2008]<2,>=1.3\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting httpcore<0.17.0,>=0.15.0\n",
            "  Downloading httpcore-0.16.1-py3-none-any.whl (68 kB)\n",
            "\u001b[K     || 68 kB 6.6 MB/s \n",
            "\u001b[?25h  Downloading httpcore-0.16.0-py3-none-any.whl (68 kB)\n",
            "\u001b[K     || 68 kB 5.2 MB/s \n",
            "\u001b[?25h  Downloading httpcore-0.15.0-py3-none-any.whl (68 kB)\n",
            "\u001b[K     || 68 kB 8.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->gradio) (2.0.1)\n",
            "Collecting mdurl~=0.1\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Collecting linkify-it-py~=1.0\n",
            "  Downloading linkify_it_py-1.0.3-py3-none-any.whl (19 kB)\n",
            "Collecting mdit-py-plugins\n",
            "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
            "\u001b[K     || 50 kB 4.3 MB/s \n",
            "\u001b[?25hCollecting uc-micro-py\n",
            "  Downloading uc_micro_py-1.0.1-py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gradio) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gradio) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gradio) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gradio) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->gradio) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->gradio) (2022.6)\n",
            "Collecting bcrypt>=3.1.3\n",
            "  Downloading bcrypt-4.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (593 kB)\n",
            "\u001b[K     || 593 kB 68.2 MB/s \n",
            "\u001b[?25hCollecting cryptography>=2.5\n",
            "  Downloading cryptography-38.0.4-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n",
            "\u001b[K     || 4.0 MB 61.1 MB/s \n",
            "\u001b[?25hCollecting pynacl>=1.0.1\n",
            "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "\u001b[K     || 856 kB 52.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.8/dist-packages (from cryptography>=2.5->paramiko->gradio) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.12->cryptography>=2.5->paramiko->gradio) (2.21)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->gradio) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->gradio) (1.24.3)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.8/dist-packages (from uvicorn->gradio) (7.1.2)\n",
            "Building wheels for collected packages: ffmpy, python-multipart\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4711 sha256=c7ee369982bd40176062bac22915bbb688d1fd80170a568ee9abe1a8bc746110\n",
            "  Stored in directory: /root/.cache/pip/wheels/ff/5b/59/913b443e7369dc04b61f607a746b6f7d83fb65e2e19fcc958d\n",
            "  Building wheel for python-multipart (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-multipart: filename=python_multipart-0.0.5-py3-none-any.whl size=31678 sha256=6b318f17c1c2b81349bd8062d5d1588bbdd9716b963e99fa1643ac352fae5a3a\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/fc/1c/cf980e6413d3ee8e70cd8f39e2366b0f487e3e221aeb452eb0\n",
            "Successfully built ffmpy python-multipart\n",
            "Installing collected packages: sniffio, mdurl, uc-micro-py, rfc3986, markdown-it-py, h11, anyio, starlette, pynacl, mdit-py-plugins, linkify-it-py, httpcore, cryptography, bcrypt, websockets, uvicorn, python-multipart, pydub, pycryptodome, paramiko, orjson, httpx, ffmpy, fastapi, gradio\n",
            "Successfully installed anyio-3.6.2 bcrypt-4.0.1 cryptography-38.0.4 fastapi-0.88.0 ffmpy-0.3.0 gradio-3.13.0 h11-0.12.0 httpcore-0.15.0 httpx-0.23.1 linkify-it-py-1.0.3 markdown-it-py-2.1.0 mdit-py-plugins-0.3.3 mdurl-0.1.2 orjson-3.8.3 paramiko-2.12.0 pycryptodome-3.16.0 pydub-0.25.1 pynacl-1.5.0 python-multipart-0.0.5 rfc3986-1.5.0 sniffio-1.3.0 starlette-0.22.0 uc-micro-py-1.0.1 uvicorn-0.20.0 websockets-10.4\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 844
        },
        "id": "MS2I7ywOlI_E",
        "outputId": "0514c036-40ca-4975-dcaa-5339eb5b63e9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gradio/inputs.py:256: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/gradio/deprecation.py:40: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
            "  warnings.warn(value)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7860, \"/\", \"100%\", 500, false, window.element)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label - Probability\n",
            "Pneumonia: 0.31117573380470276\n",
            "Normal: 0.6888242661952972\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "\n",
        "def classify_object_alexnet(numpy_image):\n",
        "  img = Image.fromarray(np.uint8(numpy_image)).convert('RGB')\n",
        "\n",
        "  preprocess = transforms.Compose(\n",
        "    [\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor()\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  input_img_preprocessed = preprocess(img)\n",
        "  input_img_under_batch = torch.unsqueeze(input_img_preprocessed, 0)\n",
        "\n",
        "  num_classes = 1\n",
        "  criterion = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  with torch.no_grad():\n",
        "    model = CustomNetwork(num_classes=num_classes, loss_fn=criterion, device=device, threshold=0.5, backbone='alexnet')\n",
        "    # model_path = 'logs/1/alexnet/lr_0.001/checkpoints/best_model.pt'\n",
        "    # model.load_state_dict(torch.load(model_path)[\"model_state_dict\"])\n",
        "    pred = model(input_img_under_batch)\n",
        "    pred_prob = 1 / (1 + torch.exp(-pred))\n",
        "    pred_class = (pred_prob > 0.5).float()\n",
        "\n",
        "  output_dictionary = {}\n",
        "  print(\"Label - Probability\")\n",
        "  entry = {\"Pneumonia\": pred_prob.item(),\n",
        "           \"Normal\": 1-pred_prob.item()}\n",
        "  output_dictionary.update(entry)\n",
        "\n",
        "  for key,val in output_dictionary.items():\n",
        "    print(f'{key}: {val}')\n",
        "  \n",
        "  return output_dictionary \n",
        "\n",
        "\n",
        "webcam = gr.inputs.Image(shape=(224, 224), source=\"webcam\")\n",
        "gr.Interface(fn=classify_object_alexnet, inputs=\"image\", outputs=\"label\").launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}